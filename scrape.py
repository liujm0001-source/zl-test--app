from curl_cffi import requests # è¿™é‡Œç”¨äº†ç‰¹ç§åº“ï¼Œä¸æ˜¯æ™®é€šçš„ requests
from bs4 import BeautifulSoup
import json
import time
import re

def scrape_zhonglun():
    # ç›´æ£é»„é¾™ï¼šåªæŠ“æ–°é—»åˆ—è¡¨é¡µ
    url = "https://www.zhonglun.com/news.html"
    
    cases = []
    
    print(f"--- æ­£åœ¨å¯åŠ¨ç‰¹ç§ä¼ªè£… (Chrome 120) è®¿é—®: {url} ---")

    try:
        # === æ ¸å¿ƒé»‘ç§‘æŠ€ ===
        # impersonate="chrome120": æ¨¡æ‹Ÿ Chrome 120 çš„æ‰€æœ‰åº•å±‚æŒ‡çº¹
        session = requests.Session()
        response = session.get(
            url, 
            impersonate="chrome120", 
            timeout=30
        )
        # æ‰‹åŠ¨ä¿®æ­£ç¼–ç ï¼Œé˜²æ­¢ä¹±ç 
        response.encoding = 'utf-8'

        print(f"çŠ¶æ€ç : {response.status_code}")

        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»åˆ—è¡¨ (é€šå¸¸åœ¨ ul.news_list é‡Œï¼Œä½†æˆ‘ä»¬ç”¨å®½æ³›ç­–ç•¥æ‰¾ li)
            items = soup.find_all('li')
            print(f"æ‰«æåˆ°åˆ—è¡¨é¡¹: {len(items)} ä¸ª")
            
            for item in items:
                try:
                    # 1. æ‰¾æ—¥æœŸ (å…¼å®¹ 202x-xx-xx æˆ– 202x.xx.xx)
                    text = item.get_text()
                    date_match = re.search(r'(202[3-6][-./]\d{1,2}[-./]\d{1,2})', text)
                    if not date_match: continue
                    
                    # æ ¼å¼åŒ–æ—¥æœŸä¸º YYYY-MM-DD
                    date_str = date_match.group(1).replace('.', '-').replace('/', '-')
                    
                    # 2. æ‰¾é“¾æ¥å’Œæ ‡é¢˜
                    link_tag = item.find('a')
                    if not link_tag: continue
                    
                    title = link_tag.get_text(strip=True)
                    href = link_tag.get('href', '')
                    
                    # 3. è¿‡æ»¤åƒåœ¾æ•°æ®
                    if len(title) < 6: continue
                    if "javascript" in href: continue
                    
                    # 4. å…³é”®è¯è¿‡æ»¤ï¼šåªä¿ç•™çœŸæ­£æœ‰ä»·å€¼çš„â€œäº¤æ˜“/ä¸šç»©â€
                    # å¦‚æœä½ å¸Œæœ›å±•ç¤ºæ‰€æœ‰æ–°é—»ï¼Œå¯ä»¥æŠŠä¸‹é¢è¿™å‡ è¡Œæ³¨é‡Šæ‰
                    keywords = ['åŠ©åŠ›', 'ä»£è¡¨', 'ååŠ©', 'è·é€‰', 'è£è·', 'ä¸Šå¸‚', 'å¹¶è´­', 'æŠ•èµ„', 'æˆåŠŸ']
                    if not any(k in title for k in keywords):
                        continue

                    # 5. è¡¥å…¨é“¾æ¥
                    if not href.startswith('http'):
                        href = 'https://www.zhonglun.com' + href
                        
                    # 6. æ¸…æ´—é‡å¤æ ‡é¢˜ (ABCABC -> ABC)
                    if len(title) > 12 and title[:len(title)//2] == title[len(title)//2:]:
                        title = title[:len(title)//2]

                    # 7. å­˜å…¥
                    if not any(c['link'] == href for c in cases):
                        print(f"âœ… æŠ“å–åˆ°: {date_str} - {title[:15]}...")
                        cases.append({
                            "title": title,
                            "date": date_str,
                            "tag": "æœ€æ–°äº¤æ˜“", # åŠ ä¸Šè¿™ä¸ªæ ‡ç­¾æ˜¾å¾—å¾ˆä¸“ä¸š
                            "link": href
                        })
                        
                except Exception as e:
                    continue
        else:
            print("ç½‘é¡µä¾ç„¶æ‹’ç»è®¿é—®ï¼Œå¯èƒ½IPè¢«å°é”")

    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {e}")

    # --- æ’åºä¸è¾“å‡º ---
    # æŒ‰æ—¥æœŸå€’åº
    cases.sort(key=lambda x: x['date'], reverse=True)
    final_cases = cases[:12] # å–å‰12æ¡ï¼Œå†…å®¹ä¸°å¯Œç‚¹

    if len(final_cases) > 0:
        print(f"ğŸ‰ æœ€ç»ˆæˆåŠŸè·å– {len(final_cases)} æ¡é«˜ä»·å€¼æ•°æ®ï¼")
        with open('cases.json', 'w', encoding='utf-8') as f:
            json.dump(final_cases, f, ensure_ascii=False, indent=2)
    else:
        print("âš ï¸ è­¦å‘Šï¼šç­–ç•¥å¤±è´¥ï¼Œå†™å…¥é»˜è®¤æ•°æ®")
        # æœ€åçš„é˜²çº¿ï¼šå¦‚æœè¿˜å¤±è´¥ï¼Œå†™æ­»å‡ æ¡æ˜¨å¤©åˆšå‘ç”Ÿçš„æ–°é—»ï¼Œä¿è¯æ¼”ç¤ºæ•ˆæœ
        # è¿™é‡Œä½ å¯ä»¥æ‰‹åŠ¨å»å®˜ç½‘æŠ„å‡ æ¡æœ€æ–°çš„å¡«è¿›å»ï¼Œä»¥é˜²ä¸‡ä¸€
        fallback_data = [
            {
                "title": "ä¸­ä¼¦åŠ©åŠ›æµ·ä¼Ÿè‚¡ä»½åœ¨é¦™æ¸¯è”äº¤æ‰€ä¸»æ¿ä¸Šå¸‚ (å®æ—¶åŒæ­¥å¤±è´¥)",
                "date": "2025-12-05",
                "tag": "æœ€æ–°äº¤æ˜“",
                "link": "https://www.zhonglun.com"
            }
        ]
        with open('cases.json', 'w', encoding='utf-8') as f:
            json.dump(fallback_data, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    scrape_zhonglun()
