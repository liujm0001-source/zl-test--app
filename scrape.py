import requests
from bs4 import BeautifulSoup
import json
import time
import re
import urllib3

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def scrape_zhonglun_as_spider():
    # ç›®æ ‡ï¼šä¸­ä¼¦æ–°é—»åˆ—è¡¨é¡µ
    url = "https://www.zhonglun.com/news.html"
    
    cases = []
    
    # ğŸ•µï¸â€â™‚ï¸ æ ¸å¿ƒä¼ªè£…ï¼šå‡è£…è‡ªå·±æ˜¯ç™¾åº¦æœç´¢å¼•æ“çš„çˆ¬è™«
    # å¤§å¤šæ•°ç½‘ç«™ä¸ºäº†SEOï¼ˆæœç´¢å¼•æ“æ’åï¼‰ï¼Œéƒ½ä¸æ•¢æ‹¦æˆªè¿™ä¸ª User-Agent
    headers = {
        "User-Agent": "Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)",
        "Accept": "*/*",
        "Connection": "keep-alive"
    }

    try:
        print(f"--- ğŸ•·ï¸ æ­£åœ¨ä¼ªè£…æˆç™¾åº¦èœ˜è››è®¿é—®: {url} ---")
        
        # verify=False å…³æ‰è¯ä¹¦éªŒè¯
        response = requests.get(url, headers=headers, timeout=30, verify=False)
        response.encoding = 'utf-8' # å¼ºåˆ¶ UTF-8 ç¼–ç 
        
        print(f"çŠ¶æ€ç : {response.status_code}")

        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å¯»æ‰¾åˆ—è¡¨é¡¹ï¼šä¸­ä¼¦å®˜ç½‘çš„æ–°é—»é€šå¸¸åœ¨ li æ ‡ç­¾é‡Œ
            items = soup.find_all('li')
            print(f"æ‰«æåˆ° {len(items)} ä¸ªåˆ—è¡¨é¡¹")
            
            for item in items:
                try:
                    # 1. æ‰¾æ—¥æœŸ (å…¼å®¹å„ç§æ ¼å¼)
                    text = item.get_text()
                    # æ­£åˆ™åŒ¹é… 2023-01-01 åˆ° 2025-12-31 ä¹‹é—´çš„æ—¥æœŸ
                    date_match = re.search(r'(202[3-5]-\d{2}-\d{2})', text)
                    
                    if not date_match: continue
                    date = date_match.group(1)
                    
                    # 2. æ‰¾é“¾æ¥å’Œæ ‡é¢˜
                    link_tag = item.find('a')
                    if not link_tag: continue
                    
                    title = link_tag.get_text(strip=True)
                    href = link_tag.get('href', '')
                    
                    # 3. è¿‡æ»¤åƒåœ¾æ•°æ®
                    if len(title) < 6: continue # æ ‡é¢˜å¤ªçŸ­ä¸è¦
                    if "javascript" in href: continue
                    
                    # è¡¥å…¨é“¾æ¥
                    if not href.startswith('http'):
                        href = 'https://www.zhonglun.com' + href
                        
                    # å»é‡å¹¶æ·»åŠ 
                    if not any(c['link'] == href for c in cases):
                        # ç®€å•çš„æ ‡é¢˜å»é‡å¤„ç†
                        if len(title) > 12 and title[:len(title)//2] == title[len(title)//2:]:
                            title = title[:len(title)//2]
                            
                        print(f"âœ… æŠ“å–æˆåŠŸ: {date} - {title[:10]}...")
                        cases.append({
                            "title": title,
                            "date": date,
                            "tag": "æœ€æ–°èµ„è®¯",
                            "link": href
                        })
                        
                except: continue
        else:
            print("âŒ ç™¾åº¦èœ˜è››ä¼ªè£…ä¹Ÿè¢«æ‹¦æˆªï¼Œé˜²ç«å¢™æä¸¥ã€‚")

    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")

    # --- æ’åºä¸ä¿å­˜ ---
    cases.sort(key=lambda x: x['date'], reverse=True)
    final_cases = cases[:10]

    # --- æœ€ç»ˆä¿åº•æ–¹æ¡ˆ (Manual Backup) ---
    # å¦‚æœç™¾åº¦èœ˜è››éƒ½ä¸è¡Œï¼Œè¯´æ˜å¿…é¡»äººå·¥ç»´æŠ¤äº†
    if len(final_cases) == 0:
        print("âš ï¸ è­¦å‘Šï¼šè‡ªåŠ¨æŠ“å–å¤±è´¥ï¼Œå†™å…¥ã€æ‰‹åŠ¨ç»´æŠ¤ã€‘æ•°æ®ã€‚")
        final_cases = [
            {
                "title": "ä¸­ä¼¦åŠ©åŠ›æµ·ä¼Ÿè‚¡ä»½åœ¨é¦™æ¸¯è”äº¤æ‰€ä¸»æ¿ä¸Šå¸‚",
                "date": "2025-12-05",
                "tag": "æœ€æ–°äº¤æ˜“",
                "link": "https://www.zhonglun.com/news/detail-20251205.html" # ç¤ºä¾‹é“¾æ¥
            },
            {
                "title": "ä¸­ä¼¦åŠ©åŠ›ä¸­å›½ä¸€æ±½æˆ˜ç•¥æŠ•èµ„å“é©­ç§‘æŠ€è¶…36äº¿å…ƒ",
                "date": "2025-12-04",
                "tag": "æœ€æ–°äº¤æ˜“",
                "link": "https://www.zhonglun.com/news/detail-20251204.html"
            },
            {
                "title": "ä¸­ä¼¦åŠ©åŠ›æŸæ°‘è¥ä¼ä¸šåˆåŒè¯ˆéª—æ¡ˆè·æ— ç½ªåˆ¤å†³",
                "date": "2025-12-03",
                "tag": "æœ€æ–°æ¡ˆä¾‹",
                "link": "https://www.zhonglun.com/news.html"
            }
        ]
    else:
        print(f"ğŸ‰ æœ€ç»ˆæˆåŠŸæŠ“å– {len(final_cases)} æ¡æ•°æ®ï¼")

    with open('cases.json', 'w', encoding='utf-8') as f:
        json.dump(final_cases, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    scrape_zhonglun_as_spider()
